{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json.decoder import JSONDecodeError\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "import time\n",
    "\n",
    "#Specify API Credentials in .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "CLIENT_ID = os.environ.get('CLIENT_ID')\n",
    "ACCESS_TOKEN = os.environ.get('ACCESS_TOKEN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Getting Clips from past Month ---\n",
    "\n",
    "def find_all_lsf_clips_past_month() -> pd.DataFrame:\n",
    "    #Parameters for query\n",
    "    endpoint = \"https://api.pushshift.io/reddit/search/submission/\"\n",
    "    end_range = 1622256944 #Set this value to UTC Epoch from 1 month ago\n",
    "    before = 9999999999\n",
    "    after = \"1m\"\n",
    "\n",
    "    df = pd.DataFrame(columns=[\"Title\", \"Created_UTC\", \"URL\", \"Post_Link\"]) #Initalize df to store clip info\n",
    "\n",
    "    #Loop because max return is 100 and we need to query until we hit the month end\n",
    "    while(True):\n",
    "        #Query pushshift for reddit submissions in timeframe\n",
    "        PARAMS = {\"subreddit\": \"LivestreamFail\", \"User-Agent\": \"LSF Analysis by u/Kgersh\", \"before\": before, \"after\": after, \"size\": 100, \"sort\": \"desc\"}\n",
    "        request = requests.get(endpoint, params=PARAMS)\n",
    "        time.sleep(1) #Pushshift has ratelimits\n",
    "\n",
    "        #Attempt to parse response, if not wait and try again\n",
    "        try:\n",
    "            response = request.json()\n",
    "        except JSONDecodeError as e:\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "\n",
    "        #Conditions for when there are no remaining posts, break loop\n",
    "        if \"data\" not in response:\n",
    "            print(\"Data not found, breaking\")\n",
    "            break\n",
    "\n",
    "        objects = response[\"data\"]\n",
    "        if(len(objects) == 0):\n",
    "            print(\"Length of objects is 0, breaking\")\n",
    "            break\n",
    "\n",
    "        #For each clip check if it is in range and make sure it was not deleted or removed then append to dataframe\n",
    "        for obj in objects:\n",
    "            if(obj[\"created_utc\"] < end_range):\n",
    "                print(obj[\"created_utc\"], \"End of date range, returning\")\n",
    "                return df\n",
    "            if \"https://clips.twitch.tv\" in obj[\"url\"]:\n",
    "                if((obj[\"selftext\"] == \"[removed]\") or (obj[\"selftext\"] == \"[deleted]\") or (\"removed_by_category\" in obj)):\n",
    "                    print(\"post check failed...\")\n",
    "                    break\n",
    "                df = df.append({\"Title\": obj[\"title\"], \"Created_UTC\": obj[\"created_utc\"], \"URL\": obj[\"url\"], \"Post_Link\": obj[\"full_link\"]}, ignore_index=True)\n",
    "        \n",
    "        #The next query will use this before value to paginate through posts\n",
    "        before = objects[-1][\"created_utc\"] - 1\n",
    "    return df\n",
    "\n",
    "clips_df = find_all_lsf_clips_past_month()\n",
    "\n",
    "#Write df to csv if needed\n",
    "#clips_df.to_csv(\"lsf_clips_month.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Getting Channels for each Clip ---\n",
    "\n",
    "#Read df back from csv if needed\n",
    "#clips_df = pd.read_csv(\"lsf_clips_month.csv\")\n",
    "\n",
    "#Queries Twitch API for channel from a clip -- Used in df.apply()\n",
    "def find_channel_from_clip(x):\n",
    "    #Parameters for query\n",
    "    clip_url = x[\"URL\"]\n",
    "    uri = clip_url.split(\".tv/\")[-1]\n",
    "    slug = uri.split(\"?\")[0] \n",
    "    endpoint = \"https://api.twitch.tv/helix/clips\"\n",
    "    HEADERS = {\"Client-Id\": CLIENT_ID, \"Authorization\": f\"Bearer {ACCESS_TOKEN}\"}\n",
    "    PARAMS = {\"id\": slug}\n",
    "\n",
    "    #Ask Twitch for info from clip\n",
    "    request = requests.get(endpoint, headers=HEADERS, params=PARAMS)\n",
    "    time.sleep(1) #Space out queries, optional but good practice\n",
    "\n",
    "    #Attempt to parse reponse, otherwise clip must have been deleted\n",
    "    try: \n",
    "        return request.json()[\"data\"][0][\"broadcaster_name\"] #Json path of channel name from response\n",
    "    except IndexError as e:\n",
    "        print(\"Clip Deleted\")\n",
    "        return \"Deleted\"\n",
    "\n",
    "#Add channels to each clip entry\n",
    "clips_df[\"Channel\"] = clips_df.apply(find_channel_from_clip, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# --- Getting Follower for each Channel ---\n",
    "#Really this should have been done after I made the channels dataframe lower down but oh well\n",
    "#Save channels we have already hit to minimize API queries\n",
    "found_channels = {}\n",
    "\n",
    "#Queries Twitch API for followers from a channel -- Used in df.apply()\n",
    "def get_followers_for_channel(x):\n",
    "    channel = x[\"Channel\"]\n",
    "\n",
    "    #Check if we've already found the followers for that channel\n",
    "    if channel in found_channels:\n",
    "        return found_channels[channel]\n",
    "\n",
    "    #Before getting followers we need the user_id of the channel\n",
    "    endpoint = \"https://api.twitch.tv/helix/users\"\n",
    "    HEADERS = {\"Client-Id\": CLIENT_ID, \"Authorization\": f\"Bearer {ACCESS_TOKEN}\"}\n",
    "    PARAMS = {\"login\": channel}\n",
    "    request = requests.get(endpoint, headers=HEADERS, params=PARAMS)\n",
    "\n",
    "    time.sleep(0.2) #Again not required but good practice\n",
    "\n",
    "    #Initialize user_id to 0 in case we don't find one \n",
    "    user_id = 0\n",
    "\n",
    "    #Attempt to parse request for the user_id of the channel otherwise just return 0 followers\n",
    "    try: \n",
    "        user_id = request.json()[\"data\"][0][\"id\"]\n",
    "    except IndexError as e:\n",
    "        print(request.json())\n",
    "        print(\"Id not found\")\n",
    "        return user_id\n",
    "    except KeyError as e:\n",
    "        print(request.json())\n",
    "        print(\"Id not found\")\n",
    "        return user_id\n",
    "    \n",
    "    #Now that we have the user_id we can ask for their follower count\n",
    "    endpoint = \"https://api.twitch.tv/helix/users/follows\"\n",
    "    PARAMS = {\"to_id\": user_id}\n",
    "    request = requests.get(endpoint, headers=HEADERS, params=PARAMS)\n",
    "\n",
    "    #Attempt to parse request for follower count otherwise return 0 followers\n",
    "    try: \n",
    "        found_channels[channel] = request.json()[\"total\"]\n",
    "        return request.json()[\"total\"]\n",
    "    except IndexError as e:\n",
    "        found_channels[channel] = request.json()[\"total\"]\n",
    "        print(request.json())\n",
    "        print(\"Id not found\")\n",
    "        return 0\n",
    "\n",
    "#Add followers to each clip entry\n",
    "clips_df[\"Follower Count\"] = clips_df.apply(get_followers_for_channel, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generating Final Data ----\n",
    "\n",
    "#Now that we have all the clip and channel info, we can do the 'by channel' analysis\n",
    "\n",
    "#Find the number of times each channel appears in the clips df\n",
    "occurences = collections.Counter(clips_df[\"Channel\"].tolist())\n",
    "del occurences[\"Deleted\"]\n",
    "\n",
    "#Create new df to store channels, clip #, and follower count\n",
    "channel_df = pd.DataFrame(columns=[\"Channel\", \"LSF Clip Count\", \"Follower Count\"])\n",
    "channel_df[\"Channel\"] = occurences.keys()\n",
    "channel_df = channel_df.set_index(\"Channel\")\n",
    "\n",
    "#Add clip # to df\n",
    "channel_df[\"LSF Clip Count\"] = occurences.values()\n",
    "\n",
    "#Add follower count to df\n",
    "for channel in occurences.keys():\n",
    "    channel_df.at[channel, \"Follower Count\"] = clips_df.loc[clips_df[\"Channel\"] == channel].iloc[0][\"Follower Count\"]\n",
    "\n",
    "#Dump to new csv\n",
    "channel_df.to_csv(\"lsf_analysis.csv\")\n"
   ]
  }
 ]
}